{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGP34F8W8prY",
        "outputId": "9cd8283e-7005-40e0-a4a9-d404b957d11e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.6.1\n",
            "  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 15.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (4.11.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (1.21.6)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 10.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (2019.12.20)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 43.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (21.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.6.1) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.6.1) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.6.1) (3.0.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.1) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.1) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.1) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.1) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=e67f178f744595f1bf95066b19ab944fc3ffc1664ae6e75cc4a1ba0cf31bcfd8\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.53 tokenizers-0.10.3 transformers-4.6.1\n"
          ]
        }
      ],
      "source": [
        "#dependency according to https://github.com/salesforce/CodeT5#dependency\n",
        "!pip install transformers==4.6.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6qTOjORmKSAs"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "from transformers import RobertaTokenizer, T5ForConditionalGeneration\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9sKwlJISWr9E"
      },
      "outputs": [],
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bef6wkLRKSEL",
        "outputId": "3cf9ed71-2cc6-4bb2-82f7-017fb6eda432"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "data_path = '/content/gdrive/My Drive/Final project_DeepLearning'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Af40FeZMLYgd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(data_path+'/java-corpus/token_completion')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMOoAdwKViOj"
      },
      "outputs": [],
      "source": [
        "#preprocess the data\n",
        "!python preprocess_java.py --base_dir=token_completion --output_dir=token_completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDNG0pukToHP",
        "outputId": "aa4dd2a5-027c-44fd-9721-2fe8c8077848"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dev.txt  test.json  train.txt\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-qN7IdQHQmmH"
      },
      "outputs": [],
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.summ_len = summ_len\n",
        "        self.text = self.data.code_\n",
        "        self.ctext = self.data.code_\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ctext = str(self.ctext[index])\n",
        "        ctext = ' '.join(ctext.split())\n",
        "\n",
        "        text = str(self.text[index])\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        # source = self.tokenizer.batch_encode_plus([self.text], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n",
        "        # target = self.tokenizer.batch_encode_plus([self.otext], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n",
        "        # print([self.text][0])\n",
        "        source = self.tokenizer([text], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n",
        "        target = self.tokenizer([ctext], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n",
        "\n",
        "        source_ids = source['input_ids'].squeeze()\n",
        "        source_mask = source['attention_mask'].squeeze()\n",
        "        target_ids = target['input_ids'].squeeze()\n",
        "        target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'source_ids': source_ids.to(dtype=torch.long), \n",
        "            'source_mask': source_mask.to(dtype=torch.long), \n",
        "            'target_ids': target_ids.to(dtype=torch.long),\n",
        "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mxzN2CqGQxaE"
      },
      "outputs": [],
      "source": [
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    train_loss = []\n",
        "    torch.save(model,'my_checkpoint.pth.tar')\n",
        "    torch.save(model, data_path+'my_checkpoint.pth.tar')\n",
        "    model.train()\n",
        "    for _,data in enumerate(loader, 0):\n",
        "        y = data['target_ids'].to(device, dtype = torch.long)\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        lm_labels = y[:, 1:].clone().detach()\n",
        "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
        "        loss = outputs[0]\n",
        "        train_loss.append(loss.item())\n",
        "        if _%100==0:\n",
        "          print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    print(\"=> Saving checkpoint\")        \n",
        "    return train_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "IOyRevhuRW_i"
      },
      "outputs": [],
      "source": [
        "def validate(epoch, tokenizer, model, device, loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    final_df = pd.DataFrame(columns = [\"Generated Text\",\"Actual Text\"])\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(loader, 0):\n",
        "            y = data['target_ids'].to(device, dtype = torch.long)\n",
        "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "            generated_ids = model.generate(\n",
        "                input_ids = ids,\n",
        "                attention_mask = mask, \n",
        "                max_length=550, \n",
        "                num_beams=2,\n",
        "                repetition_penalty=2.5, \n",
        "                length_penalty=1.0, \n",
        "                early_stopping=True\n",
        "                )\n",
        "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
        "            if _%100==0:\n",
        "                print(f'Completed {_}')\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            actuals.extend(target)\n",
        "            values = [list(i) for i in zip(preds,target)]\n",
        "\n",
        "            #create a temporary dataframe for the predicted texts of the current iteration\n",
        "            df = pd.DataFrame(values, columns=[\"Generated Text\",\"Actual Text\"])\n",
        "\n",
        "            #Concat to the original dataframe\n",
        "            final_df = pd.concat([final_df, df], ignore_index=True)\n",
        "            # final_df['Generated Text'] = predictions\n",
        "            # final_df['Actual Text'] = actuals\n",
        "            final_df.to_csv('predictions_token.csv')\n",
        "    return predictions, actuals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "H9YGRUQDEb38"
      },
      "outputs": [],
      "source": [
        "Train_batch  = 8\n",
        "Valid_batch = 8\n",
        "epochs = 1\n",
        "val_epochs = 1\n",
        "learning_rate = 1e-4\n",
        "seed = 42\n",
        "max_len = 512\n",
        "output_len = 150"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaANV0IBRXCZ",
        "outputId": "cd550e0a-cb07-4427-a307-de5054389df5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  return func(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN Dataset: (12928, 3)\n",
            "VAL Dataset: (7151, 3)\n"
          ]
        }
      ],
      "source": [
        "# Set random seeds and deterministic pytorch for reproducibility\n",
        "torch.manual_seed(seed) # pytorch random seed\n",
        "np.random.seed(seed) # numpy random seed\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# tokenzier for encoding the text\n",
        "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-small')\n",
        "\n",
        "train_data = pd.read_csv('train.txt', sep='<s>', header = None)\n",
        "dev_data = pd.read_csv('dev.txt', sep='<s>', header = None)\n",
        "\n",
        "#apply <s> at the beginning\n",
        "train_data.columns = ['idx', 'code']\n",
        "train_data['code_'] = train_data['code'].apply(lambda x:'<s> '+x)\n",
        "\n",
        "dev_data.columns = ['idx', 'code']\n",
        "dev_data['code_'] = dev_data['code'].apply(lambda x:'<s> '+x)\n",
        "\n",
        "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
        "print(\"VAL Dataset: {}\".format(dev_data.shape))\n",
        "\n",
        "#calling the dataset class \n",
        "training_set = Dataset(train_data, tokenizer, max_len, output_len)\n",
        "val_set = Dataset(dev_data, tokenizer, max_len, output_len)\n",
        "\n",
        "# Defining the parameters for creation of dataloaders\n",
        "train_params = {\n",
        "    'batch_size': Train_batch,\n",
        "    'shuffle': True,\n",
        "    'num_workers': 2\n",
        "    }\n",
        "\n",
        "val_params = {\n",
        "    'batch_size': Valid_batch,\n",
        "    'shuffle': False,\n",
        "    'num_workers': 2\n",
        "    }\n",
        "\n",
        "# Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "val_loader = DataLoader(val_set, **val_params)\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-small')\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRW4Ja3j4u1s",
        "outputId": "b0108a23-e9d0-4d55-e58f-15691fc5f0f5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss:  11.65631103515625\n",
            "Epoch: 0, Loss:  0.1891576647758484\n",
            "Epoch: 0, Loss:  0.10172505676746368\n",
            "Epoch: 0, Loss:  0.08996093273162842\n",
            "Epoch: 0, Loss:  0.053098827600479126\n",
            "Epoch: 0, Loss:  0.0554431714117527\n",
            "Epoch: 0, Loss:  0.0714089646935463\n",
            "Epoch: 0, Loss:  0.05402199178934097\n",
            "Epoch: 0, Loss:  0.04175151139497757\n",
            "Epoch: 0, Loss:  0.043756019324064255\n",
            "Epoch: 0, Loss:  0.05515299364924431\n",
            "Epoch: 0, Loss:  0.03199891000986099\n",
            "Epoch: 0, Loss:  0.04672214388847351\n",
            "Epoch: 0, Loss:  0.03528938814997673\n",
            "Epoch: 0, Loss:  0.03430168703198433\n",
            "Epoch: 0, Loss:  0.044024012982845306\n",
            "Epoch: 0, Loss:  0.05026824027299881\n",
            "=> Saving checkpoint\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    training_loss = train(epoch, tokenizer, model, device, training_loader, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(training_loss)\n",
        "plt.xlabel('Iterations (plotted for every 100 iterations)')\n",
        "plt.ylabel('Training loss ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "052QtbHMXn1N",
        "outputId": "8eeba8cd-e82b-4ecc-9034-a80d08e6e25c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Training loss ')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdTJoJkmNcH-"
      },
      "outputs": [],
      "source": [
        "test = pd.read_json('test.json', lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "d3HUzkxAOG40",
        "outputId": "1745ed74-a9db-4122-f6ed-e41b162185ee"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-fe80b691-8a5c-408c-be41-d9934a98d1e3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>input</th>\n",
              "      <th>gt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>51</td>\n",
              "      <td>&lt;s&gt; import threading &lt;EOL&gt; import IECore &lt;EOL&gt;...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>40753</td>\n",
              "      <td>&lt;s&gt; import re , operator &lt;EOL&gt; def str_find_al...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>32889</td>\n",
              "      <td>&lt;s&gt; import unittest &lt;EOL&gt; import pymel . inter...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>49349</td>\n",
              "      <td>&lt;s&gt; import time &lt;EOL&gt; import logging &lt;EOL&gt; imp...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>27038</td>\n",
              "      <td>&lt;s&gt; import os &lt;EOL&gt; import os . path as osp &lt;E...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>13140</td>\n",
              "      <td>&lt;s&gt; \"\"\"&lt;STR_LIT&gt;\"\"\" &lt;EOL&gt; try : &lt;EOL&gt; from cPi...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>9254</td>\n",
              "      <td>&lt;s&gt; from pypy . rpython . lltypesystem import ...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>24242</td>\n",
              "      <td>&lt;s&gt; import itertools &lt;EOL&gt; import time &lt;EOL&gt; c...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>44099</td>\n",
              "      <td>&lt;s&gt; \"\"\"&lt;STR_LIT&gt;\"\"\" &lt;EOL&gt; import os &lt;EOL&gt; impo...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>9273</td>\n",
              "      <td>&lt;s&gt; from pypy . translator . platform import P...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fe80b691-8a5c-408c-be41-d9934a98d1e3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fe80b691-8a5c-408c-be41-d9934a98d1e3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fe80b691-8a5c-408c-be41-d9934a98d1e3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         id                                              input gt\n",
              "0        51  <s> import threading <EOL> import IECore <EOL>...   \n",
              "1     40753  <s> import re , operator <EOL> def str_find_al...   \n",
              "2     32889  <s> import unittest <EOL> import pymel . inter...   \n",
              "3     49349  <s> import time <EOL> import logging <EOL> imp...   \n",
              "4     27038  <s> import os <EOL> import os . path as osp <E...   \n",
              "...     ...                                                ... ..\n",
              "9995  13140  <s> \"\"\"<STR_LIT>\"\"\" <EOL> try : <EOL> from cPi...   \n",
              "9996   9254  <s> from pypy . rpython . lltypesystem import ...   \n",
              "9997  24242  <s> import itertools <EOL> import time <EOL> c...   \n",
              "9998  44099  <s> \"\"\"<STR_LIT>\"\"\" <EOL> import os <EOL> impo...   \n",
              "9999   9273  <s> from pypy . translator . platform import P...   \n",
              "\n",
              "[10000 rows x 3 columns]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ct77p7lOOran"
      },
      "outputs": [],
      "source": [
        "test.columns\n",
        "test.rename(columns={'input': 'code_'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlKaPJvMO-R5",
        "outputId": "52d22539-728a-4078-8fcf-6ede8f4f89f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0        \n",
              "1        \n",
              "2        \n",
              "3        \n",
              "4        \n",
              "       ..\n",
              "9995     \n",
              "9996     \n",
              "9997     \n",
              "9998     \n",
              "9999     \n",
              "Name: gt, Length: 10000, dtype: object"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test['gt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bnr5ukGWdvL",
        "outputId": "373df2ec-41dd-47f2-d8ff-2d10ddbdb9fe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1818: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  next_indices = next_tokens // vocab_size\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "Completed 300\n"
          ]
        }
      ],
      "source": [
        "#-------------------------------------------------------------------\n",
        "# Testing and saving the results to a dataframe - line completion task\n",
        "#-------------------------------------------------------------------\n",
        "\n",
        "vt_set = Dataset(test, tokenizer, max_len, output_len)\n",
        "test_params = {\n",
        "    'batch_size': 8,\n",
        "    'shuffle': False,\n",
        "    'num_workers': 2\n",
        "    }\n",
        "test_loader = DataLoader(vt_set, **val_params)\n",
        "for epoch in range(val_epochs):\n",
        "    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
        "    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Y7pkSqJFVLR"
      },
      "outputs": [],
      "source": [
        "#---------------------------------------------------------------\n",
        "#result metrics\n",
        "#---------------------------------------------------------------\n",
        "from nltk.metrics import edit_distance    \n",
        "final_df[\"distance\"] = final_df.loc[:, [\"Generated Text\",\"Actual Text\"]].apply(lambda x: edit_distance(*x), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVDvtblOfyO4"
      },
      "outputs": [],
      "source": [
        "final_df[\"exact_match\"] = final_df.loc[:, [\"Generated Text\",\"Actual Text\"]].apply(lambda x: 1 (if x[\"Generated Text\"].split()==x[\"Actual Text\"].split()) else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7hQzETNFVOZ",
        "outputId": "b44c3cd9-3745-4bbe-eeea-9d2aa7b456e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Edit distance:  113.64019018319117\n"
          ]
        }
      ],
      "source": [
        "print(\"Edit distance: \", final_df[\"distance\"].mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaUaS2F6dovg"
      },
      "outputs": [],
      "source": [
        "final_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNl-5zAhg-tS"
      },
      "outputs": [],
      "source": [
        "final_df.to_csv('predictions.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nbX930KLQcF0"
      },
      "outputs": [],
      "source": [
        "model = torch.load(data_path+'my_checkpoint.pth.tar', map_location=torch.device('cpu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSYEzA2AWd0i",
        "outputId": "e2415b74-5dcb-4e0f-fa6d-5eb81940da00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user.send(user.name) print(f'hello!')\n"
          ]
        }
      ],
      "source": [
        "#Example inference\n",
        "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-small')\n",
        "\n",
        "text = \"def (user): print(f'hello!')\"\n",
        "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# simply generate one code span\n",
        "generated_ids = model.generate(input_ids, max_length = 50)   #line_generation\n",
        "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGMR2-YSHLYk"
      },
      "source": [
        "# Token level inferencing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHrC-c42Fgkf",
        "outputId": "89be432e-da2a-4bef-87fa-e2b686506bd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-05-09 13:51:11--  https://zenodo.org/record/3628665/files/java_test_pre\n",
            "Resolving zenodo.org (zenodo.org)... 137.138.76.77\n",
            "Connecting to zenodo.org (zenodo.org)|137.138.76.77|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26969670 (26M) [application/octet-stream]\n",
            "Saving to: ‘test.txt’\n",
            "\n",
            "test.txt            100%[===================>]  25.72M  1.28MB/s    in 41s     \n",
            "\n",
            "2022-05-09 13:51:54 (640 KB/s) - ‘test.txt’ saved [26969670/26969670]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#getting the test file for token completion task\n",
        "\n",
        "!wget -O test.txt https://zenodo.org/record/3628665/files/java_test_pre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzXCdR__RXE0",
        "outputId": "8ec160da-9854-48b3-a097-9d5b1adad9df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  return func(*args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "#--------------------------------------------------------------------------------\n",
        "#token level inferencing\n",
        "#--------------------------------------------------------------------------------\n",
        "test_token = pd.read_csv(\"test.txt\", sep='<s>', header = None)\n",
        "test_token.columns = ['idx', 'code']\n",
        "test_token['code_'] = test_token['code'].apply(lambda x:'<s> '+x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ZpXYEdLnH6Qa",
        "outputId": "9d1f7a7d-24a5-4ee2-d7fd-cfaef1012edf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-593a998f-a54b-4faa-964c-e10853fdafca\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>idx</th>\n",
              "      <th>code</th>\n",
              "      <th>code_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>package org . vaadin . teemu . clara . demo ;...</td>\n",
              "      <td>&lt;s&gt;  package org . vaadin . teemu . clara . de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>package org . vaadin . teemu . clara . demo ;...</td>\n",
              "      <td>&lt;s&gt;  package org . vaadin . teemu . clara . de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>package org . vaadin . teemu . clara ; import...</td>\n",
              "      <td>&lt;s&gt;  package org . vaadin . teemu . clara ; im...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>package org . vaadin . teemu . clara . util ;...</td>\n",
              "      <td>&lt;s&gt;  package org . vaadin . teemu . clara . ut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>package org . vaadin . teemu . clara ; import...</td>\n",
              "      <td>&lt;s&gt;  package org . vaadin . teemu . clara ; im...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8263</th>\n",
              "      <td>NaN</td>\n",
              "      <td>package journal . io . util ; import java . u...</td>\n",
              "      <td>&lt;s&gt;  package journal . io . util ; import java...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8264</th>\n",
              "      <td>NaN</td>\n",
              "      <td>package journal . io . util ; import java . i...</td>\n",
              "      <td>&lt;s&gt;  package journal . io . util ; import java...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8265</th>\n",
              "      <td>NaN</td>\n",
              "      <td>package journal . io . api ; import java . io...</td>\n",
              "      <td>&lt;s&gt;  package journal . io . api ; import java ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8266</th>\n",
              "      <td>NaN</td>\n",
              "      <td>package journal . io . api ; import java . io...</td>\n",
              "      <td>&lt;s&gt;  package journal . io . api ; import java ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8267</th>\n",
              "      <td>NaN</td>\n",
              "      <td>package journal . io . api ; import java . io...</td>\n",
              "      <td>&lt;s&gt;  package journal . io . api ; import java ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8268 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-593a998f-a54b-4faa-964c-e10853fdafca')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-593a998f-a54b-4faa-964c-e10853fdafca button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-593a998f-a54b-4faa-964c-e10853fdafca');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      idx                                               code  \\\n",
              "0     NaN   package org . vaadin . teemu . clara . demo ;...   \n",
              "1     NaN   package org . vaadin . teemu . clara . demo ;...   \n",
              "2     NaN   package org . vaadin . teemu . clara ; import...   \n",
              "3     NaN   package org . vaadin . teemu . clara . util ;...   \n",
              "4     NaN   package org . vaadin . teemu . clara ; import...   \n",
              "...   ...                                                ...   \n",
              "8263  NaN   package journal . io . util ; import java . u...   \n",
              "8264  NaN   package journal . io . util ; import java . i...   \n",
              "8265  NaN   package journal . io . api ; import java . io...   \n",
              "8266  NaN   package journal . io . api ; import java . io...   \n",
              "8267  NaN   package journal . io . api ; import java . io...   \n",
              "\n",
              "                                                  code_  \n",
              "0     <s>  package org . vaadin . teemu . clara . de...  \n",
              "1     <s>  package org . vaadin . teemu . clara . de...  \n",
              "2     <s>  package org . vaadin . teemu . clara ; im...  \n",
              "3     <s>  package org . vaadin . teemu . clara . ut...  \n",
              "4     <s>  package org . vaadin . teemu . clara ; im...  \n",
              "...                                                 ...  \n",
              "8263  <s>  package journal . io . util ; import java...  \n",
              "8264  <s>  package journal . io . util ; import java...  \n",
              "8265  <s>  package journal . io . api ; import java ...  \n",
              "8266  <s>  package journal . io . api ; import java ...  \n",
              "8267  <s>  package journal . io . api ; import java ...  \n",
              "\n",
              "[8268 rows x 3 columns]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSTYzbGXRXHR",
        "outputId": "bffd27ed-595a-4dc1-a861-67275a8f5212"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1818: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  next_indices = next_tokens // vocab_size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "Completed 300\n",
            "Completed 400\n",
            "Completed 500\n",
            "Completed 600\n",
            "Completed 700\n",
            "Completed 800\n",
            "Completed 900\n",
            "Completed 1000\n"
          ]
        }
      ],
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-small')\n",
        "\n",
        "test_token_set = Dataset(test_token, tokenizer, max_len, output_len)\n",
        "test_params = {\n",
        "    'batch_size': 8,\n",
        "    'shuffle': False,\n",
        "    'num_workers': 2\n",
        "    }\n",
        "test_loader = DataLoader(test_token_set, **test_params)\n",
        "for epoch in range(val_epochs):\n",
        "    predictions, actuals = validate(epoch, tokenizer, model, device, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = pd.read_csv(\"predictions_token.csv\", index_col=0)"
      ],
      "metadata": {
        "id": "iTfU75hE29oe"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t39OFwzO3Lbr",
        "outputId": "309971aa-e1ee-4968-e7e5-40c0a4ca6387"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Generated Text', 'Actual Text'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = list(tmp['Generated Text'])\n",
        "actuals = list(tmp['Actual Text'])"
      ],
      "metadata": {
        "id": "oW5XSqZi3PNN"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------\n",
        "#accuracy of predictions..\n",
        "#Based on the evaluator.py script from CodeXGLUE\n",
        "#--------------------------------------------------------------\n",
        "\n",
        "total = 0\n",
        "correct = 0.0\n",
        "for pred, gt in zip(predictions, actuals):\n",
        "    pred = pred.split()\n",
        "    gt = gt.split()\n",
        "    for x, y in zip(pred, gt):\n",
        "        if y not in [\"<s>\", \"</s>\", \"<EOL>\", \"<pad>\"]:\n",
        "            total += 1\n",
        "            if x == y:\n",
        "                correct += 1\n",
        "print((f\"Total {total} tokens, accuracy: {round(correct/total*100, 2)}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGm-wGQXngH2",
        "outputId": "43da2dce-5ac9-4521-e3bf-dcaf31745ba4"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total 204001 tokens, accuracy: 0.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"<s> import json <EOL> json . load ( f ) </s>\"\n",
        "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# simply generate one code span\n",
        "generated_ids = model.generate(input_ids, max_length = 10)   #token completion example\n",
        "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FspxneHp5aGt",
        "outputId": "d7807df9-e4c9-42fe-cf78-889de5d29aa6"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def parse_json ( f )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cllL0UcARXKl"
      },
      "outputs": [],
      "source": [
        "#------------------------------------------------------\n",
        "#understanding the data (should've been done at the beginning)\n",
        "#------------------------------------------------------\n",
        "\n",
        "data = pd.read_csv(\"train.csv\", sep='<s>', header = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdssNuyAdwwV"
      },
      "outputs": [],
      "source": [
        "data['CodeLength'] = data['code'].apply(lambda x:len(x.split()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjLRvSVJeP7o",
        "outputId": "aa2460f7-3327-4486-a583-b55ce60ed730"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum code length  264199\n",
            "Minimum code length  6\n",
            "mean code length  1212.4225711633662\n"
          ]
        }
      ],
      "source": [
        "#check the highest, lowest and the mean of the lengths of the code\n",
        "print(\"Maximum code length \", max(data['CodeLength']))\n",
        "print(\"Minimum code length \", min(data['CodeLength']))\n",
        "print(\"mean code length \", data['CodeLength'].mean())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "references:\n",
        "https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb\n",
        "https://towardsdatascience.com/guide-to-fine-tuning-text-generation-models-gpt-2-gpt-neo-and-t5-dc5de6b3bc5e\n",
        "https://shivanandroy.com/fine-tune-t5-transformer-with-pytorch/\n",
        "https://codeutility.org/huggingface-t5-transformer-model-how-to-prep-a-custom-dataset-for-fine-tuning/\n",
        "'''"
      ],
      "metadata": {
        "id": "qH2UW4VKnlly"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Final project_DL_CodeT5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}