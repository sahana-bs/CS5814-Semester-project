{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Final_Project_DL_CodeGPT.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d446b29d26a6490e897bc9bd236cf13f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_60bb4059d18d4974b5f15f03fba59e76","IPY_MODEL_e08d70cbce58452db0db773ab9347c62","IPY_MODEL_175d64e1aaab429fa2621b84682eda2d"],"layout":"IPY_MODEL_5bdf97201e744c7abe78d4e01ec14ecd"}},"60bb4059d18d4974b5f15f03fba59e76":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_997fbaa4b7544103a30736d393ed90c8","placeholder":"​","style":"IPY_MODEL_3cac0a592abc45bdb96e8b52b884dd6a","value":"Downloading: 100%"}},"e08d70cbce58452db0db773ab9347c62":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_73216a8eaa2046208c02b75aace63745","max":898669,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1e47cb5fa5b345959581cd8ba86ebdde","value":898669}},"175d64e1aaab429fa2621b84682eda2d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ade12d02ba04493381d789823984f081","placeholder":"​","style":"IPY_MODEL_2c8e616d52fa446aadf6f8aebb6e01cb","value":" 899k/899k [00:00&lt;00:00, 2.58MB/s]"}},"5bdf97201e744c7abe78d4e01ec14ecd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"997fbaa4b7544103a30736d393ed90c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3cac0a592abc45bdb96e8b52b884dd6a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"73216a8eaa2046208c02b75aace63745":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e47cb5fa5b345959581cd8ba86ebdde":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ade12d02ba04493381d789823984f081":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c8e616d52fa446aadf6f8aebb6e01cb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c23ccdd4169d4702951f73d6f3453d34":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2bfe23493c2f43158a72d5d18b0600b3","IPY_MODEL_0b3a58b64a1443d690febe4ea2700261","IPY_MODEL_033a6ec5550e4e2eb33c8ff2ef216b7e"],"layout":"IPY_MODEL_cfed92773b9645b3a433730eda3d8086"}},"2bfe23493c2f43158a72d5d18b0600b3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_74afbecd77b448c890bfcbf9f363fb8b","placeholder":"​","style":"IPY_MODEL_32893af82c044a86901750eaf6be4e34","value":"Downloading: 100%"}},"0b3a58b64a1443d690febe4ea2700261":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ef0189cef8b49278330e957c739e232","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9a88df77d0134704a5a20f49cb516c08","value":456318}},"033a6ec5550e4e2eb33c8ff2ef216b7e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_02ce7d869bd646018b940962542fd2c9","placeholder":"​","style":"IPY_MODEL_10255969e81e424aafe13c1f1ac2f344","value":" 456k/456k [00:00&lt;00:00, 969kB/s]"}},"cfed92773b9645b3a433730eda3d8086":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74afbecd77b448c890bfcbf9f363fb8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32893af82c044a86901750eaf6be4e34":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ef0189cef8b49278330e957c739e232":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a88df77d0134704a5a20f49cb516c08":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"02ce7d869bd646018b940962542fd2c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10255969e81e424aafe13c1f1ac2f344":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"586fadcd48854588a5ee0f4ebb39c30f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e30647874a2e4950acbaf189b2db1a04","IPY_MODEL_1d14c82f6c864dd38cc19d6da24e6035","IPY_MODEL_cff93cabc002426d8380480085e04629"],"layout":"IPY_MODEL_1ca97913d5ba4ea9bada475e59ed1770"}},"e30647874a2e4950acbaf189b2db1a04":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91c567bc9599486b89418ab548bfe58f","placeholder":"​","style":"IPY_MODEL_b83ccb5772224a519469b9ee0a2ee12a","value":"Downloading: 100%"}},"1d14c82f6c864dd38cc19d6da24e6035":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ee824d747f142f89be11049566287f5","max":45,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8e2e2dc97f204734911e0f9ca448663c","value":45}},"cff93cabc002426d8380480085e04629":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1226136b8f804033a0d901ce2427a039","placeholder":"​","style":"IPY_MODEL_ab484535173046338caea01536fb478e","value":" 45.0/45.0 [00:00&lt;00:00, 1.25kB/s]"}},"1ca97913d5ba4ea9bada475e59ed1770":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91c567bc9599486b89418ab548bfe58f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b83ccb5772224a519469b9ee0a2ee12a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7ee824d747f142f89be11049566287f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e2e2dc97f204734911e0f9ca448663c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1226136b8f804033a0d901ce2427a039":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab484535173046338caea01536fb478e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5676c128d4744b90a982cfc2712e64d3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_619d39c1851f48fca7d85179d078d45b","IPY_MODEL_75e46f0a46ba4b758ba47ac990c642d7","IPY_MODEL_cc926e1bedf7440eb91589494dafbd3a"],"layout":"IPY_MODEL_ee840bb0c5994cfe865ce54ec2b895a8"}},"619d39c1851f48fca7d85179d078d45b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9b64457f39f40608c293e20768028ed","placeholder":"​","style":"IPY_MODEL_8f5c768983d1423d9e965440331b8367","value":"Downloading: 100%"}},"75e46f0a46ba4b758ba47ac990c642d7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b923875d04ef427f8b8609cd8fd0a1ce","max":358,"min":0,"orientation":"horizontal","style":"IPY_MODEL_acbf201629ba46f7a7f299ddb3231ad8","value":358}},"cc926e1bedf7440eb91589494dafbd3a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e9dfb9ea48a42caab12c9cf999130ee","placeholder":"​","style":"IPY_MODEL_a1858bfa04404aecaac65fee7b130708","value":" 358/358 [00:00&lt;00:00, 8.21kB/s]"}},"ee840bb0c5994cfe865ce54ec2b895a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9b64457f39f40608c293e20768028ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f5c768983d1423d9e965440331b8367":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b923875d04ef427f8b8609cd8fd0a1ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"acbf201629ba46f7a7f299ddb3231ad8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6e9dfb9ea48a42caab12c9cf999130ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1858bfa04404aecaac65fee7b130708":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a1d3386af2154578b3e873a2950edc6a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4971e84d620c4ae48b519003d12ae809","IPY_MODEL_477c73d2bdc74e9e8ea611b1b5ff3092","IPY_MODEL_26d891d37b5b41db9398a5a532ac6d9c"],"layout":"IPY_MODEL_388453548c244698800894dbfe149c3e"}},"4971e84d620c4ae48b519003d12ae809":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0eb82ecd403a473f86f252eeb20dd365","placeholder":"​","style":"IPY_MODEL_eb43fbe05dfc42ba91382963a4b18e4a","value":"Downloading: 100%"}},"477c73d2bdc74e9e8ea611b1b5ff3092":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea0ec7190ba145b98039cf9f48cf0c48","max":177,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d2081eb8a6504c87b0431a5a817ad669","value":177}},"26d891d37b5b41db9398a5a532ac6d9c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e95a6150bfb4c15ab261af6c928a22b","placeholder":"​","style":"IPY_MODEL_b95af414fad9420b818f1a566943d06c","value":" 177/177 [00:00&lt;00:00, 4.01kB/s]"}},"388453548c244698800894dbfe149c3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0eb82ecd403a473f86f252eeb20dd365":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb43fbe05dfc42ba91382963a4b18e4a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ea0ec7190ba145b98039cf9f48cf0c48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2081eb8a6504c87b0431a5a817ad669":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4e95a6150bfb4c15ab261af6c928a22b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b95af414fad9420b818f1a566943d06c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c87366c3a7b4c0ca518c9f07975bddf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e8c988f807e54e199c20dee5a49078c8","IPY_MODEL_53369cb73c87422795b0ce083e6c5cb8","IPY_MODEL_156518a8394f41c4b389a42455f05645"],"layout":"IPY_MODEL_1be8da8be3ba4336baf38a16f26fbab6"}},"e8c988f807e54e199c20dee5a49078c8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a3a92e507ea4956b9130437a59a7cf4","placeholder":"​","style":"IPY_MODEL_990abb5cafe04614ab25081f67182a63","value":"Downloading: 100%"}},"53369cb73c87422795b0ce083e6c5cb8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_697d6e8001924de7ba8a17bc69d7a613","max":720,"min":0,"orientation":"horizontal","style":"IPY_MODEL_df75faa551454146a9ab7db755a4bb32","value":720}},"156518a8394f41c4b389a42455f05645":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_75401a626e6b4a7f8330731d0be1c621","placeholder":"​","style":"IPY_MODEL_e11dfb1394f94f418bd1ccbec5bed0d5","value":" 720/720 [00:00&lt;00:00, 15.5kB/s]"}},"1be8da8be3ba4336baf38a16f26fbab6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a3a92e507ea4956b9130437a59a7cf4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"990abb5cafe04614ab25081f67182a63":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"697d6e8001924de7ba8a17bc69d7a613":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df75faa551454146a9ab7db755a4bb32":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"75401a626e6b4a7f8330731d0be1c621":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e11dfb1394f94f418bd1ccbec5bed0d5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c80081cfbbdd4b2e8adb13dc0f56b7d8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d2f65c2a1ed343429e31399e32eb8def","IPY_MODEL_ca990f53e24d4cc38ca48c1e56133562","IPY_MODEL_c74dfdd30af0485a88e790b92acbb51e"],"layout":"IPY_MODEL_4df7091f1bdd4a7780871320b2096648"}},"d2f65c2a1ed343429e31399e32eb8def":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_01c5853fe13842cdb3b997b0c65409db","placeholder":"​","style":"IPY_MODEL_01b06331a5ce4e1898b9bbfc31f6bf2e","value":"Downloading: 100%"}},"ca990f53e24d4cc38ca48c1e56133562":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bff57f31c89940c3900a1704fba56fc9","max":510387898,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a2c940b44eb541c094f1436adc786e83","value":510387898}},"c74dfdd30af0485a88e790b92acbb51e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c586df605975432aace71034ed2b51a1","placeholder":"​","style":"IPY_MODEL_c5bc51cac87c4d3da3101e67be1b7015","value":" 510M/510M [00:11&lt;00:00, 43.2MB/s]"}},"4df7091f1bdd4a7780871320b2096648":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01c5853fe13842cdb3b997b0c65409db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01b06331a5ce4e1898b9bbfc31f6bf2e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bff57f31c89940c3900a1704fba56fc9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2c940b44eb541c094f1436adc786e83":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c586df605975432aace71034ed2b51a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5bc51cac87c4d3da3101e67be1b7015":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["#dependency according to https://github.com/salesforce/CodeT5#dependency\n","!pip install transformers==4.6.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kGP34F8W8prY","executionInfo":{"status":"ok","timestamp":1652121582653,"user_tz":240,"elapsed":3512,"user":{"displayName":"Ian Chang","userId":"12935946874489503365"}},"outputId":"176c2de5-200d-44e2-bc38-4095bcc615f4"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers==4.6.1 in /usr/local/lib/python3.7/dist-packages (4.6.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (21.3)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (0.0.53)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (4.11.3)\n","Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (0.0.8)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (3.6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (1.21.6)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (0.10.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (4.64.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (2019.12.20)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.6.1) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.6.1) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.6.1) (3.0.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.1) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.1) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.1) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.1) (2021.10.8)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.1) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.1) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.1) (7.1.2)\n"]}]},{"cell_type":"code","source":["#imports\n","from transformers import RobertaTokenizer, T5ForConditionalGeneration\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler"],"metadata":{"id":"6qTOjORmKSAs","executionInfo":{"status":"ok","timestamp":1652121592703,"user_tz":240,"elapsed":10053,"user":{"displayName":"Ian Chang","userId":"12935946874489503365"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'"],"metadata":{"id":"9sKwlJISWr9E","executionInfo":{"status":"ok","timestamp":1652121592924,"user_tz":240,"elapsed":226,"user":{"displayName":"Ian Chang","userId":"12935946874489503365"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive/')\n","data_path = '/content/gdrive/My Drive/CS5814/Project'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bef6wkLRKSEL","executionInfo":{"status":"ok","timestamp":1652121617336,"user_tz":240,"elapsed":17980,"user":{"displayName":"Ian Chang","userId":"12935946874489503365"}},"outputId":"2cf68abe-fad6-48b9-e4db-d6bc138e1118"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n"]}]},{"cell_type":"code","source":["import os\n","os.chdir(data_path+'/javaCorpus/token_completion')"],"metadata":{"id":"Af40FeZMLYgd","executionInfo":{"status":"ok","timestamp":1652121618324,"user_tz":240,"elapsed":991,"user":{"displayName":"Ian Chang","userId":"12935946874489503365"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["#preprocess the data\n","!python preprocess_java.py --base_dir=token_completion --output_dir=token_completion"],"metadata":{"id":"yMOoAdwKViOj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652121618710,"user_tz":240,"elapsed":388,"user":{"displayName":"Ian Chang","userId":"12935946874489503365"}},"outputId":"2e0cae3f-4c67-4eba-f438-c9cf088ff3a3"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["python3: can't open file 'preprocess_java.py': [Errno 2] No such file or directory\n"]}]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FDNG0pukToHP","executionInfo":{"status":"ok","timestamp":1652121618710,"user_tz":240,"elapsed":5,"user":{"displayName":"Ian Chang","userId":"12935946874489503365"}},"outputId":"5ed6dd9b-c615-4044-fa6b-69fb217c207a"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["dev.txt  my_checkpoint.pth.tar\tpredictions.csv  test.json  train.txt\n"]}]},{"cell_type":"code","source":["class Dataset(Dataset):\n","    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n","        self.tokenizer = tokenizer\n","        self.data = dataframe\n","        self.source_len = source_len\n","        self.summ_len = summ_len\n","        self.text = self.data.code_\n","        self.ctext = self.data.code_\n","\n","    def __len__(self):\n","        return len(self.text)\n","\n","    def __getitem__(self, index):\n","        ctext = str(self.ctext[index])\n","        ctext = ' '.join(ctext.split())\n","\n","        text = str(self.text[index])\n","        text = ' '.join(text.split())\n","\n","        # source = self.tokenizer.batch_encode_plus([self.text], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n","        # target = self.tokenizer.batch_encode_plus([self.otext], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n","        # print([self.text][0])\n","        source = self.tokenizer([text], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n","        target = self.tokenizer([ctext], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n","\n","        source_ids = source['input_ids'].squeeze()\n","        source_mask = source['attention_mask'].squeeze()\n","        target_ids = target['input_ids'].squeeze()\n","        target_mask = target['attention_mask'].squeeze()\n","\n","        return {\n","            'source_ids': source_ids.to(dtype=torch.long), \n","            'source_mask': source_mask.to(dtype=torch.long), \n","            'target_ids': target_ids.to(dtype=torch.long),\n","            'target_ids_y': target_ids.to(dtype=torch.long)\n","        }"],"metadata":{"id":"-qN7IdQHQmmH","executionInfo":{"status":"ok","timestamp":1652121618711,"user_tz":240,"elapsed":3,"user":{"displayName":"Ian Chang","userId":"12935946874489503365"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def train(epoch, tokenizer, model, device, loader, optimizer):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model = model.to(device)\n","    train_loss = []\n","    torch.save(model,'my_checkpoint.pth.tar')\n","    torch.save(model, data_path+'my_checkpoint.pth.tar')\n","    model.train()\n","    for _,data in enumerate(loader, 0):\n","        y = data['target_ids'].to(device, dtype = torch.long)\n","        y_ids = y[:, :-1].contiguous()\n","        lm_labels = y[:, 1:].clone().detach()\n","        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n","        ids = data['source_ids'].to(device, dtype = torch.long)\n","        mask = data['source_mask'].to(device, dtype = torch.long)\n","\n","        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n","        loss = outputs[0]\n","        train_loss.append(loss.item())\n","        if _%100==0:\n","          print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    \n","    print(\"=> Saving checkpoint\")        \n","    return train_loss"],"metadata":{"id":"mxzN2CqGQxaE","executionInfo":{"status":"ok","timestamp":1652121618880,"user_tz":240,"elapsed":4,"user":{"displayName":"Ian Chang","userId":"12935946874489503365"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def validate(epoch, tokenizer, model, device, loader):\n","    model.eval()\n","    predictions = []\n","    actuals = []\n","    with torch.no_grad():\n","        for _, data in enumerate(loader, 0):\n","            y = data['target_ids'].to(device, dtype = torch.long)\n","            ids = data['source_ids'].to(device, dtype = torch.long)\n","            mask = data['source_mask'].to(device, dtype = torch.long)\n","\n","            generated_ids = model.generate(\n","                input_ids = ids,\n","                attention_mask = mask, \n","                max_length=550, \n","                num_beams=2,\n","                repetition_penalty=2.5, \n","                length_penalty=1.0, \n","                early_stopping=True\n","                )\n","            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n","            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n","            if _%100==0:\n","                print(f'Completed {_}')\n","\n","            predictions.extend(preds)\n","            actuals.extend(target)\n","    return predictions, actuals"],"metadata":{"id":"IOyRevhuRW_i","executionInfo":{"status":"ok","timestamp":1652121618880,"user_tz":240,"elapsed":3,"user":{"displayName":"Ian Chang","userId":"12935946874489503365"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["Train_batch  = 8\n","Valid_batch = 8\n","epochs = 1\n","val_epochs = 1\n","learning_rate = 1e-4\n","seed = 42\n","max_len = 512\n","output_len = 150"],"metadata":{"id":"H9YGRUQDEb38","executionInfo":{"status":"ok","timestamp":1652121618880,"user_tz":240,"elapsed":3,"user":{"displayName":"Ian Chang","userId":"12935946874489503365"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Set random seeds and deterministic pytorch for reproducibility\n","torch.manual_seed(seed) # pytorch random seed\n","np.random.seed(seed) # numpy random seed\n","torch.backends.cudnn.deterministic = True\n","\n","# tokenzier for encoding the text\n","tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/CodeGPT-small-java-adaptedGPT2\")\n","\n","train_data = pd.read_csv('train.txt', sep='<s>', header = None)\n","dev_data = pd.read_csv('dev.txt', sep='<s>', header = None)\n","\n","#apply <s> at the beginning\n","train_data.columns = ['idx', 'code']\n","train_data['code_'] = train_data['code'].apply(lambda x:'<s> '+x)\n","\n","dev_data.columns = ['idx', 'code']\n","dev_data['code_'] = dev_data['code'].apply(lambda x:'<s> '+x)\n","\n","print(\"TRAIN Dataset: {}\".format(train_data.shape))\n","print(\"VAL Dataset: {}\".format(dev_data.shape))\n","\n","#calling the dataset class \n","training_set = Dataset(train_data, tokenizer, max_len, output_len)\n","val_set = Dataset(dev_data, tokenizer, max_len, output_len)\n","\n","# Defining the parameters for creation of dataloaders\n","train_params = {\n","    'batch_size': Train_batch,\n","    'shuffle': True,\n","    'num_workers': 2\n","    }\n","\n","val_params = {\n","    'batch_size': Valid_batch,\n","    'shuffle': False,\n","    'num_workers': 2\n","    }\n","\n","# Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n","training_loader = DataLoader(training_set, **train_params)\n","val_loader = DataLoader(val_set, **val_params)\n","\n","model = T5ForConditionalGeneration.from_pretrained('microsoft/CodeGPT-small-java-adaptedGPT2')\n","model = model.to(device)\n","\n","# Defining the optimizer that will be used to tune the weights of the network in the training session. \n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":452,"referenced_widgets":["d446b29d26a6490e897bc9bd236cf13f","60bb4059d18d4974b5f15f03fba59e76","e08d70cbce58452db0db773ab9347c62","175d64e1aaab429fa2621b84682eda2d","5bdf97201e744c7abe78d4e01ec14ecd","997fbaa4b7544103a30736d393ed90c8","3cac0a592abc45bdb96e8b52b884dd6a","73216a8eaa2046208c02b75aace63745","1e47cb5fa5b345959581cd8ba86ebdde","ade12d02ba04493381d789823984f081","2c8e616d52fa446aadf6f8aebb6e01cb","c23ccdd4169d4702951f73d6f3453d34","2bfe23493c2f43158a72d5d18b0600b3","0b3a58b64a1443d690febe4ea2700261","033a6ec5550e4e2eb33c8ff2ef216b7e","cfed92773b9645b3a433730eda3d8086","74afbecd77b448c890bfcbf9f363fb8b","32893af82c044a86901750eaf6be4e34","5ef0189cef8b49278330e957c739e232","9a88df77d0134704a5a20f49cb516c08","02ce7d869bd646018b940962542fd2c9","10255969e81e424aafe13c1f1ac2f344","586fadcd48854588a5ee0f4ebb39c30f","e30647874a2e4950acbaf189b2db1a04","1d14c82f6c864dd38cc19d6da24e6035","cff93cabc002426d8380480085e04629","1ca97913d5ba4ea9bada475e59ed1770","91c567bc9599486b89418ab548bfe58f","b83ccb5772224a519469b9ee0a2ee12a","7ee824d747f142f89be11049566287f5","8e2e2dc97f204734911e0f9ca448663c","1226136b8f804033a0d901ce2427a039","ab484535173046338caea01536fb478e","5676c128d4744b90a982cfc2712e64d3","619d39c1851f48fca7d85179d078d45b","75e46f0a46ba4b758ba47ac990c642d7","cc926e1bedf7440eb91589494dafbd3a","ee840bb0c5994cfe865ce54ec2b895a8","c9b64457f39f40608c293e20768028ed","8f5c768983d1423d9e965440331b8367","b923875d04ef427f8b8609cd8fd0a1ce","acbf201629ba46f7a7f299ddb3231ad8","6e9dfb9ea48a42caab12c9cf999130ee","a1858bfa04404aecaac65fee7b130708","a1d3386af2154578b3e873a2950edc6a","4971e84d620c4ae48b519003d12ae809","477c73d2bdc74e9e8ea611b1b5ff3092","26d891d37b5b41db9398a5a532ac6d9c","388453548c244698800894dbfe149c3e","0eb82ecd403a473f86f252eeb20dd365","eb43fbe05dfc42ba91382963a4b18e4a","ea0ec7190ba145b98039cf9f48cf0c48","d2081eb8a6504c87b0431a5a817ad669","4e95a6150bfb4c15ab261af6c928a22b","b95af414fad9420b818f1a566943d06c","3c87366c3a7b4c0ca518c9f07975bddf","e8c988f807e54e199c20dee5a49078c8","53369cb73c87422795b0ce083e6c5cb8","156518a8394f41c4b389a42455f05645","1be8da8be3ba4336baf38a16f26fbab6","0a3a92e507ea4956b9130437a59a7cf4","990abb5cafe04614ab25081f67182a63","697d6e8001924de7ba8a17bc69d7a613","df75faa551454146a9ab7db755a4bb32","75401a626e6b4a7f8330731d0be1c621","e11dfb1394f94f418bd1ccbec5bed0d5","c80081cfbbdd4b2e8adb13dc0f56b7d8","d2f65c2a1ed343429e31399e32eb8def","ca990f53e24d4cc38ca48c1e56133562","c74dfdd30af0485a88e790b92acbb51e","4df7091f1bdd4a7780871320b2096648","01c5853fe13842cdb3b997b0c65409db","01b06331a5ce4e1898b9bbfc31f6bf2e","bff57f31c89940c3900a1704fba56fc9","a2c940b44eb541c094f1436adc786e83","c586df605975432aace71034ed2b51a1","c5bc51cac87c4d3da3101e67be1b7015"]},"id":"KaANV0IBRXCZ","executionInfo":{"status":"ok","timestamp":1652121638857,"user_tz":240,"elapsed":19980,"user":{"displayName":"Ian Chang","userId":"12935946874489503365"}},"outputId":"12efad0a-ae23-4e01-b704-cd4af0ad754d"},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d446b29d26a6490e897bc9bd236cf13f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c23ccdd4169d4702951f73d6f3453d34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/45.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"586fadcd48854588a5ee0f4ebb39c30f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/358 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5676c128d4744b90a982cfc2712e64d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/177 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1d3386af2154578b3e873a2950edc6a"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n","  return func(*args, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["TRAIN Dataset: (12928, 3)\n","VAL Dataset: (7151, 3)\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/720 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c87366c3a7b4c0ca518c9f07975bddf"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["You are using a model of type gpt2 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/510M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c80081cfbbdd4b2e8adb13dc0f56b7d8"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/CodeGPT-small-java-adaptedGPT2 were not used when initializing T5ForConditionalGeneration: ['transformer.h.9.attn.c_proj.weight', 'transformer.h.11.ln_1.bias', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.11.attn.c_attn.bias', 'transformer.h.2.attn.c_attn.bias', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.6.attn.c_attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.4.ln_1.weight', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.2.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.2.attn.bias', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.11.attn.bias', 'transformer.h.4.mlp.c_fc.bias', 'transformer.h.2.ln_1.bias', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.10.ln_2.bias', 'transformer.h.3.attn.c_attn.bias', 'transformer.h.11.mlp.c_proj.bias', 'transformer.h.5.attn.c_attn.bias', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.11.ln_2.bias', 'transformer.h.8.mlp.c_fc.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.9.attn.c_proj.bias', 'transformer.wpe.weight', 'transformer.h.1.ln_1.bias', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.1.ln_2.bias', 'transformer.h.2.mlp.c_fc.weight', 'transformer.ln_f.bias', 'transformer.h.10.ln_1.bias', 'transformer.h.7.attn.c_attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.mlp.c_fc.bias', 'transformer.h.2.mlp.c_fc.bias', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.8.ln_2.weight', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.7.mlp.c_fc.bias', 'transformer.h.8.ln_2.bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.3.mlp.c_fc.bias', 'transformer.h.10.attn.bias', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.0.ln_2.bias', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.5.ln_2.weight', 'transformer.h.10.ln_1.weight', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.1.attn.bias', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.9.ln_1.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.2.ln_2.weight', 'transformer.h.7.attn.masked_bias', 'transformer.h.0.ln_1.weight', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.8.attn.bias', 'transformer.h.6.ln_2.weight', 'transformer.h.3.ln_1.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.10.attn.c_attn.bias', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.5.ln_2.bias', 'transformer.ln_f.weight', 'transformer.h.6.ln_1.bias', 'lm_head.weight', 'transformer.h.4.ln_1.bias', 'transformer.h.9.ln_1.weight', 'transformer.h.4.ln_2.weight', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.7.attn.bias', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.1.ln_1.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.5.attn.bias', 'transformer.h.6.ln_1.weight', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.6.ln_2.bias', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.0.attn.bias', 'transformer.h.6.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.wte.weight', 'transformer.h.10.mlp.c_fc.bias', 'transformer.h.8.attn.c_attn.bias', 'transformer.h.4.attn.c_attn.bias', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.7.ln_2.bias', 'transformer.h.3.ln_1.weight', 'transformer.h.5.ln_1.weight', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.10.ln_2.weight', 'transformer.h.11.ln_2.weight', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.9.ln_2.weight', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.5.attn.masked_bias', 'transformer.h.11.ln_1.weight', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.3.ln_2.weight', 'transformer.h.0.ln_2.weight', 'transformer.h.1.ln_2.weight', 'transformer.h.5.mlp.c_fc.bias', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.2.ln_2.bias', 'transformer.h.6.mlp.c_fc.bias', 'transformer.h.1.attn.c_attn.bias', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.8.ln_1.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.7.ln_1.bias', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.9.mlp.c_fc.bias', 'transformer.h.3.ln_2.bias', 'transformer.h.7.ln_1.weight', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.2.ln_1.weight', 'transformer.h.8.ln_1.weight', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.9.ln_2.bias', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.9.attn.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.4.ln_2.bias', 'transformer.h.9.attn.c_attn.bias', 'transformer.h.1.mlp.c_fc.bias', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.5.ln_1.bias', 'transformer.h.7.ln_2.weight']\n","- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at microsoft/CodeGPT-small-java-adaptedGPT2 and are newly initialized: ['transformer.decoder.block.1.layer.1.EncDecAttention.o.weight', 'transformer.encoder.block.3.layer.0.layer_norm.weight', 'transformer.encoder.block.3.layer.0.SelfAttention.q.weight', 'transformer.decoder.block.5.layer.2.DenseReluDense.wi.weight', 'transformer.decoder.block.5.layer.2.DenseReluDense.wo.weight', 'transformer.encoder.block.3.layer.1.layer_norm.weight', 'transformer.encoder.block.0.layer.1.layer_norm.weight', 'transformer.decoder.block.1.layer.1.EncDecAttention.k.weight', 'transformer.decoder.block.3.layer.2.DenseReluDense.wi.weight', 'transformer.decoder.block.4.layer.0.SelfAttention.k.weight', 'transformer.encoder.block.5.layer.0.layer_norm.weight', 'transformer.decoder.block.3.layer.1.EncDecAttention.k.weight', 'transformer.decoder.block.3.layer.0.SelfAttention.k.weight', 'transformer.decoder.block.3.layer.0.SelfAttention.o.weight', 'transformer.decoder.block.2.layer.0.SelfAttention.v.weight', 'transformer.decoder.block.1.layer.0.SelfAttention.v.weight', 'transformer.encoder.block.5.layer.1.DenseReluDense.wo.weight', 'transformer.decoder.block.5.layer.2.layer_norm.weight', 'transformer.decoder.block.3.layer.2.DenseReluDense.wo.weight', 'transformer.encoder.block.0.layer.1.DenseReluDense.wi.weight', 'transformer.decoder.block.2.layer.2.DenseReluDense.wo.weight', 'transformer.decoder.block.4.layer.0.SelfAttention.v.weight', 'transformer.encoder.block.2.layer.1.DenseReluDense.wi.weight', 'transformer.decoder.block.0.layer.0.SelfAttention.k.weight', 'transformer.decoder.block.2.layer.1.EncDecAttention.v.weight', 'transformer.encoder.block.4.layer.0.SelfAttention.o.weight', 'transformer.encoder.block.4.layer.1.DenseReluDense.wo.weight', 'transformer.decoder.block.5.layer.0.SelfAttention.q.weight', 'transformer.decoder.block.5.layer.1.EncDecAttention.o.weight', 'transformer.decoder.block.3.layer.1.EncDecAttention.v.weight', 'transformer.decoder.block.5.layer.1.EncDecAttention.k.weight', 'transformer.decoder.block.5.layer.1.layer_norm.weight', 'transformer.decoder.block.0.layer.1.layer_norm.weight', 'transformer.shared.weight', 'transformer.encoder.block.0.layer.0.layer_norm.weight', 'transformer.decoder.block.1.layer.0.SelfAttention.k.weight', 'transformer.decoder.block.2.layer.0.SelfAttention.q.weight', 'transformer.decoder.block.0.layer.1.EncDecAttention.q.weight', 'transformer.encoder.block.0.layer.0.SelfAttention.o.weight', 'transformer.encoder.block.1.layer.0.SelfAttention.o.weight', 'transformer.decoder.block.0.layer.0.SelfAttention.o.weight', 'transformer.decoder.block.2.layer.0.SelfAttention.o.weight', 'transformer.encoder.block.0.layer.1.DenseReluDense.wo.weight', 'transformer.encoder.block.2.layer.0.SelfAttention.o.weight', 'transformer.encoder.block.2.layer.0.SelfAttention.k.weight', 'transformer.decoder.block.4.layer.1.layer_norm.weight', 'transformer.decoder.block.0.layer.2.DenseReluDense.wi.weight', 'transformer.decoder.block.5.layer.1.EncDecAttention.v.weight', 'transformer.encoder.block.1.layer.1.DenseReluDense.wi.weight', 'transformer.decoder.block.0.layer.1.EncDecAttention.v.weight', 'transformer.decoder.block.2.layer.1.EncDecAttention.q.weight', 'transformer.encoder.block.5.layer.1.layer_norm.weight', 'transformer.decoder.block.2.layer.0.SelfAttention.k.weight', 'transformer.decoder.block.2.layer.1.layer_norm.weight', 'transformer.encoder.block.5.layer.0.SelfAttention.q.weight', 'transformer.decoder.block.4.layer.1.EncDecAttention.q.weight', 'transformer.decoder.block.1.layer.0.layer_norm.weight', 'transformer.encoder.block.5.layer.0.SelfAttention.o.weight', 'transformer.decoder.block.0.layer.2.layer_norm.weight', 'transformer.decoder.block.0.layer.1.EncDecAttention.k.weight', 'transformer.decoder.block.4.layer.0.layer_norm.weight', 'transformer.decoder.final_layer_norm.weight', 'transformer.encoder.block.0.layer.0.SelfAttention.v.weight', 'transformer.encoder.block.4.layer.0.SelfAttention.v.weight', 'transformer.decoder.block.4.layer.2.DenseReluDense.wo.weight', 'transformer.encoder.block.5.layer.0.SelfAttention.v.weight', 'transformer.encoder.block.3.layer.0.SelfAttention.v.weight', 'transformer.encoder.block.4.layer.0.SelfAttention.k.weight', 'transformer.encoder.block.4.layer.0.SelfAttention.q.weight', 'transformer.decoder.block.1.layer.0.SelfAttention.q.weight', 'transformer.decoder.block.3.layer.0.layer_norm.weight', 'transformer.decoder.block.0.layer.0.SelfAttention.q.weight', 'transformer.decoder.block.3.layer.1.EncDecAttention.q.weight', 'transformer.encoder.block.0.layer.0.SelfAttention.q.weight', 'transformer.decoder.block.4.layer.2.layer_norm.weight', 'transformer.encoder.block.1.layer.0.SelfAttention.q.weight', 'transformer.encoder.block.1.layer.0.SelfAttention.k.weight', 'transformer.encoder.block.2.layer.0.layer_norm.weight', 'transformer.encoder.block.2.layer.0.SelfAttention.q.weight', 'transformer.decoder.block.3.layer.0.SelfAttention.v.weight', 'transformer.decoder.block.5.layer.0.SelfAttention.o.weight', 'transformer.encoder.block.0.layer.0.SelfAttention.k.weight', 'transformer.decoder.block.4.layer.0.SelfAttention.o.weight', 'transformer.decoder.block.3.layer.2.layer_norm.weight', 'transformer.decoder.block.5.layer.1.EncDecAttention.q.weight', 'transformer.decoder.block.4.layer.1.EncDecAttention.k.weight', 'transformer.decoder.block.1.layer.2.DenseReluDense.wo.weight', 'transformer.decoder.block.2.layer.2.layer_norm.weight', 'transformer.decoder.block.0.layer.0.SelfAttention.v.weight', 'transformer.encoder.block.1.layer.1.layer_norm.weight', 'transformer.encoder.block.4.layer.1.DenseReluDense.wi.weight', 'transformer.decoder.block.1.layer.2.layer_norm.weight', 'transformer.decoder.block.2.layer.1.EncDecAttention.k.weight', 'transformer.encoder.block.1.layer.0.layer_norm.weight', 'transformer.encoder.block.2.layer.1.DenseReluDense.wo.weight', 'transformer.decoder.block.2.layer.0.layer_norm.weight', 'transformer.encoder.block.3.layer.0.SelfAttention.o.weight', 'transformer.encoder.block.2.layer.0.SelfAttention.v.weight', 'transformer.decoder.block.1.layer.1.layer_norm.weight', 'transformer.decoder.block.2.layer.1.EncDecAttention.o.weight', 'transformer.decoder.block.5.layer.0.layer_norm.weight', 'transformer.decoder.block.2.layer.2.DenseReluDense.wi.weight', 'transformer.decoder.block.4.layer.0.SelfAttention.q.weight', 'transformer.decoder.block.0.layer.2.DenseReluDense.wo.weight', 'transformer.decoder.block.1.layer.1.EncDecAttention.v.weight', 'transformer.encoder.block.3.layer.1.DenseReluDense.wi.weight', 'transformer.decoder.block.1.layer.2.DenseReluDense.wi.weight', 'transformer.encoder.block.5.layer.1.DenseReluDense.wi.weight', 'transformer.decoder.block.0.layer.1.EncDecAttention.o.weight', 'transformer.decoder.block.1.layer.0.SelfAttention.o.weight', 'transformer.decoder.block.4.layer.1.EncDecAttention.o.weight', 'transformer.decoder.block.3.layer.0.SelfAttention.q.weight', 'transformer.decoder.block.0.layer.0.layer_norm.weight', 'transformer.encoder.block.1.layer.1.DenseReluDense.wo.weight', 'transformer.decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'transformer.encoder.final_layer_norm.weight', 'transformer.decoder.block.5.layer.0.SelfAttention.k.weight', 'transformer.encoder.block.3.layer.1.DenseReluDense.wo.weight', 'transformer.decoder.block.3.layer.1.layer_norm.weight', 'transformer.encoder.block.1.layer.0.SelfAttention.v.weight', 'transformer.decoder.block.1.layer.1.EncDecAttention.q.weight', 'transformer.decoder.block.4.layer.2.DenseReluDense.wi.weight', 'transformer.decoder.block.5.layer.0.SelfAttention.v.weight', 'transformer.encoder.block.4.layer.1.layer_norm.weight', 'transformer.encoder.block.3.layer.0.SelfAttention.k.weight', 'transformer.encoder.block.4.layer.0.layer_norm.weight', 'transformer.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'transformer.encoder.block.5.layer.0.SelfAttention.k.weight', 'transformer.encoder.block.2.layer.1.layer_norm.weight', 'transformer.decoder.block.3.layer.1.EncDecAttention.o.weight', 'transformer.decoder.block.4.layer.1.EncDecAttention.v.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["# Training loop\n","for epoch in range(epochs):\n","    training_loss = train(epoch, tokenizer, model, device, training_loader, optimizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":661},"id":"eRW4Ja3j4u1s","executionInfo":{"status":"error","timestamp":1652141515257,"user_tz":240,"elapsed":19876405,"user":{"displayName":"Ian Chang","userId":"12935946874489503365"}},"outputId":"c0930509-2644-41e0-8bcd-436731d119cb"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 0, Loss:  15.565641403198242\n","Epoch: 0, Loss:  5.332231521606445\n","Epoch: 0, Loss:  3.9460790157318115\n","Epoch: 0, Loss:  3.7325363159179688\n","Epoch: 0, Loss:  4.5346999168396\n","Epoch: 0, Loss:  3.9557387828826904\n","Epoch: 0, Loss:  3.574065685272217\n","Epoch: 0, Loss:  3.4835915565490723\n","Epoch: 0, Loss:  3.420846462249756\n","Epoch: 0, Loss:  3.0919270515441895\n","Epoch: 0, Loss:  3.487515449523926\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-39135c502d32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtraining_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-12-d1e5a134cbdb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, tokenizer, model, device, loader, optimizer)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlm_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1554\u001b[0;31m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1555\u001b[0m             )\n\u001b[1;32m   1556\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseModelOutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    994\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    997\u001b[0m                 )\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;31m# Apply Feed Forward layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;31m# clamp inf values to enable fp16 training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mforwarded_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0mforwarded_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDenseReluDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["## Training accuracy vs number of epochs\n","# training_loss = [11.594493865966797, 0.14940930902957916, 0.09707176685333252, 0.0923515036702156, 0.05420288071036339, 0.0764363557100296,0.04925357550382614,0.058485135436058044, 0.08185863494873047, 0.031726688146591187]\n","plt.plot(training_loss)\n","plt.xlabel('Iterations')\n","plt.ylabel('Training loss')"],"metadata":{"id":"uEFqoY3LDU06","executionInfo":{"status":"aborted","timestamp":1652141514401,"user_tz":240,"elapsed":646,"user":{"displayName":"Ian Chang","userId":"12935946874489503365"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test = pd.read_json('test.json', lines=True)"],"metadata":{"id":"JdTJoJkmNcH-","executionInfo":{"status":"ok","timestamp":1652141521404,"user_tz":240,"elapsed":1133,"user":{"displayName":"Ian Chang","userId":"12935946874489503365"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["test"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"d3HUzkxAOG40","executionInfo":{"status":"ok","timestamp":1652141522136,"user_tz":240,"elapsed":155,"user":{"displayName":"Ian Chang","userId":"12935946874489503365"}},"outputId":"1a892171-5fae-4c55-adda-df0fc80d5148"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["         id                                              input gt\n","0        51  <s> import threading <EOL> import IECore <EOL>...   \n","1     40753  <s> import re , operator <EOL> def str_find_al...   \n","2     32889  <s> import unittest <EOL> import pymel . inter...   \n","3     49349  <s> import time <EOL> import logging <EOL> imp...   \n","4     27038  <s> import os <EOL> import os . path as osp <E...   \n","...     ...                                                ... ..\n","9995  13140  <s> \"\"\"<STR_LIT>\"\"\" <EOL> try : <EOL> from cPi...   \n","9996   9254  <s> from pypy . rpython . lltypesystem import ...   \n","9997  24242  <s> import itertools <EOL> import time <EOL> c...   \n","9998  44099  <s> \"\"\"<STR_LIT>\"\"\" <EOL> import os <EOL> impo...   \n","9999   9273  <s> from pypy . translator . platform import P...   \n","\n","[10000 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-a2851e07-b6ef-4d20-aec7-8eb38e0e276d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>input</th>\n","      <th>gt</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>51</td>\n","      <td>&lt;s&gt; import threading &lt;EOL&gt; import IECore &lt;EOL&gt;...</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>40753</td>\n","      <td>&lt;s&gt; import re , operator &lt;EOL&gt; def str_find_al...</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>32889</td>\n","      <td>&lt;s&gt; import unittest &lt;EOL&gt; import pymel . inter...</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>49349</td>\n","      <td>&lt;s&gt; import time &lt;EOL&gt; import logging &lt;EOL&gt; imp...</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>27038</td>\n","      <td>&lt;s&gt; import os &lt;EOL&gt; import os . path as osp &lt;E...</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9995</th>\n","      <td>13140</td>\n","      <td>&lt;s&gt; \"\"\"&lt;STR_LIT&gt;\"\"\" &lt;EOL&gt; try : &lt;EOL&gt; from cPi...</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>9996</th>\n","      <td>9254</td>\n","      <td>&lt;s&gt; from pypy . rpython . lltypesystem import ...</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>9997</th>\n","      <td>24242</td>\n","      <td>&lt;s&gt; import itertools &lt;EOL&gt; import time &lt;EOL&gt; c...</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>9998</th>\n","      <td>44099</td>\n","      <td>&lt;s&gt; \"\"\"&lt;STR_LIT&gt;\"\"\" &lt;EOL&gt; import os &lt;EOL&gt; impo...</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>9999</th>\n","      <td>9273</td>\n","      <td>&lt;s&gt; from pypy . translator . platform import P...</td>\n","      <td></td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10000 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a2851e07-b6ef-4d20-aec7-8eb38e0e276d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a2851e07-b6ef-4d20-aec7-8eb38e0e276d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a2851e07-b6ef-4d20-aec7-8eb38e0e276d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["test.columns\n","test.rename(columns={'input': 'code_'}, inplace=True)"],"metadata":{"id":"Ct77p7lOOran","executionInfo":{"status":"ok","timestamp":1652141524162,"user_tz":240,"elapsed":152,"user":{"displayName":"Ian Chang","userId":"12935946874489503365"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["test['gt']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AlKaPJvMO-R5","executionInfo":{"status":"ok","timestamp":1652141525499,"user_tz":240,"elapsed":167,"user":{"displayName":"Ian Chang","userId":"12935946874489503365"}},"outputId":"019e3012-0dd0-4a7c-f097-b10914396810"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0        \n","1        \n","2        \n","3        \n","4        \n","       ..\n","9995     \n","9996     \n","9997     \n","9998     \n","9999     \n","Name: gt, Length: 10000, dtype: object"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["#-------------------------------------------------------------------\n","# Testing and saving the results to a dataframe\n","#-------------------------------------------------------------------\n","\n","val_epochs = 1\n","test_set = Dataset(test, tokenizer, max_len, output_len)\n","test_params = {\n","    'batch_size': 8,\n","    'shuffle': False,\n","    'num_workers': 2\n","    }\n","test_loader = DataLoader(test_set, **val_params)\n","for epoch in range(val_epochs):\n","    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n","    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})"],"metadata":{"id":"2bnr5ukGWdvL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#---------------------------------------------------------------\n","#result metrics\n","#---------------------------------------------------------------\n","from nltk.metrics import edit_distance    \n","final_df[\"distance\"] = final_df.loc[:, [\"Generated Text\",\"Actual Text\"]].apply(lambda x: edit_distance(*x), axis=1)"],"metadata":{"id":"5Y7pkSqJFVLR","executionInfo":{"status":"aborted","timestamp":1652141514410,"user_tz":240,"elapsed":655,"user":{"displayName":"Ian Chang","userId":"12935946874489503365"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_df[\"exact_match\"] = final_df.loc[:, [\"Generated Text\",\"Actual Text\"]].apply(lambda x: 1 (if x[\"Generated Text\"].split()==x[\"Actual Text\"].split()) else 0)"],"metadata":{"id":"gVDvtblOfyO4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Edit distance: \", final_df[\"distance\"].mean())"],"metadata":{"id":"h7hQzETNFVOZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_df"],"metadata":{"id":"TaUaS2F6dovg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_df.to_csv('predictions.csv')"],"metadata":{"id":"CNl-5zAhg-tS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = torch.load(data_path+'my_checkpoint.pth.tar')"],"metadata":{"id":"nbX930KLQcF0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Example inference\n","\n","text = \"def (user): print(f'hello <extra_id_0>!')\"\n","input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n","\n","# simply generate one code span\n","generated_ids = model.generate(input_ids, max_length = 50)   #line_generation\n","print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n"],"metadata":{"id":"fSYEzA2AWd0i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Token level inferencing"],"metadata":{"id":"_9d88UgQVGQM"}},{"cell_type":"code","source":["#getting the test file for token completion task\n","\n","!wget -O test.txt https://zenodo.org/record/3628665/files/java_test_pre"],"metadata":{"id":"cllL0UcARXKl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#--------------------------------------------------------------------------------\n","#token level inferencing\n","#--------------------------------------------------------------------------------\n","test_token = pd.read_csv(\"test.txt\", sep='<s>', header = None)\n","test_token.columns = ['idx', 'code']\n","test_token['code_'] = test_token['code'].apply(lambda x:'<s> '+x)"],"metadata":{"id":"FdssNuyAdwwV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_token"],"metadata":{"id":"XjLRvSVJeP7o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = RobertaTokenizer.from_pretrained('microsoft/CodeGPT-small-java-adaptedGPT2')\n","\n","test_token_set = Dataset(test_token, tokenizer, max_len, output_len)\n","test_params = {\n","    'batch_size': 8,\n","    'shuffle': False,\n","    'num_workers': 2\n","    }\n","test_loader = DataLoader(test_token_set, **test_params)\n","for epoch in range(val_epochs):\n","    predictions, actuals = validate(epoch, tokenizer, model, device, test_loader)"],"metadata":{"id":"niQCKXLlVQZF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tmp = pd.read_csv(\"predictions_token.csv\", index_col=0)"],"metadata":{"id":"UeqkneE_Veuq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions = list(tmp['Generated Text'])\n","actuals = list(tmp['Actual Text'])"],"metadata":{"id":"Nxr7PxYRVgWU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#-------------------------------------------------------------\n","#accuracy of predictions..\n","#Based on the evaluator.py script from CodeXGLUE\n","#--------------------------------------------------------------\n","\n","total = 0\n","correct = 0.0\n","for pred, gt in zip(predictions, actuals):\n","    pred = pred.split()\n","    gt = gt.split()\n","    for x, y in zip(pred, gt):\n","        if y not in [\"<s>\", \"</s>\", \"<EOL>\", \"<pad>\"]:\n","            total += 1\n","            if x == y:\n","                correct += 1\n","print((f\"Total {total} tokens, accuracy: {round(correct/total*100, 2)}\"))"],"metadata":{"id":"vVOIA30yViHm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = \"<s> import json <EOL> json . load ( f ) </s>\"\n","input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n","\n","# simply generate one code span\n","generated_ids = model.generate(input_ids, max_length = 10)   #token completion example\n","print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))"],"metadata":{"id":"yOR8mRGvVkf7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#------------------------------------------------------\n","#understanding the data (should've been done at the beginning)\n","#------------------------------------------------------\n","\n","data = pd.read_csv(\"train.csv\", sep='<s>', header = None)"],"metadata":{"id":"j3nXngq9VmAP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data['CodeLength'] = data['code'].apply(lambda x:len(x.split()))"],"metadata":{"id":"872H5CfCVnqv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#check the highest, lowest and the mean of the lengths of the code\n","print(\"Maximum code length \", max(data['CodeLength']))\n","print(\"Minimum code length \", min(data['CodeLength']))\n","print(\"mean code length \", data['CodeLength'].mean())"],"metadata":{"id":"kjQfc7jtVpR4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Converting to python file"],"metadata":{"id":"c99KQRcRuSca"}},{"cell_type":"code","source":["os.chdir(data_path)\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U-hC_lmBy6cP","executionInfo":{"status":"ok","timestamp":1652149782831,"user_tz":240,"elapsed":315,"user":{"displayName":"Ian Chang","userId":"12935946874489503365"}},"outputId":"f5dab426-075b-4df4-8149-af4968dce4f7"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":[" CodeCompletion-token\t\t'CS5814 Project.ipynb'\t\t  javaCorpus\n","'CS5814 Final Project.gslides'\t Final_Project_DL_CodeGPT.ipynb\n"]}]},{"cell_type":"code","source":["!jupyter nbconvert --to script Final_Project_DL_CodeGPT.ipynb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lc_aoQz3uBa9","executionInfo":{"status":"ok","timestamp":1652149787556,"user_tz":240,"elapsed":1498,"user":{"displayName":"Ian Chang","userId":"12935946874489503365"}},"outputId":"50c199b9-3a30-4ea7-ff76-9fad2500e0b0"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["[NbConvertApp] Converting notebook Final_Project_DL_CodeGPT.ipynb to script\n","[NbConvertApp] Writing 10994 bytes to Final_Project_DL_CodeGPT.txt\n"]}]}]}